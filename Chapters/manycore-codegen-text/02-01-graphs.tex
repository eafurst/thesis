\section{Graph Definitions and Preliminaries}\label{thesis:background:graphproc}
The graph abstraction stores data in a way that maintains connections and relationships between data objects.
A graph $g(V,E)$ is defined as a set of vertices $V$ and a set of edges between vertices $E$. 
An edge connecting vertices $u$ and $v$ would be referred to as $(u,v)$.
An edge can also have a weight associated with it; in this case, an edge would be defined $(u,v,w)$ where $w$ is the weight associated with the edge connecting vertices $u$ and $v$. 
Edges can be directed or undirected; an undirected edge can also be thought of as being bidirectional.
The number of edges connected to a vertex is a vertex's degree.
Graphs are often stored in compressed sparse row (CSR) format. 
We discuss this storage format in more detail in Chapter~\ref{pap:generals2020:sec:graphit}.

\section{Graph Topology}\label{thesis:background:topology}
Graph application performance is dependent on both the algorithm and the structure of the input graph.
In this thesis we consider two broad classes of graph topologies; scale-free or social network graphs and planar or road graphs.
A scale-free or social network graph has a degree distribution that follows a power-law, at least asymptotically~\cite{barabasi1999emergence}.
Most social network graphs have a small diameter and exhibit the small-world property. 
In a small-world graph, most vertices are not connected to one another, but the majority of vertices can be reached from every other vertex in a small number of hops~\cite{watts1998collective}.
Planar or road graphs have a large diameter and low average degree.
The sparsity of a graph is determined by the average degree of the graph.
All of the graphs we examine in this thesis are relatively sparse, but the planar graphs we study are the most sparse.
Both scale-free and planar graphs present their own unique challenges for optimization.
%\todo{discuss what this means in practice and tie in to real world friendships then use this citation}~\cite{milgram1967small}

\section{Graphs used in this Thesis}\label{thesis:background:graphs}
In this work, we use a diverse set of graphs for evaluation.
These graphs and their properties are listed in Table~\ref{tab:graphprop}.
A graph's topology greatly impacts a workload's characteristics and performance, so it is important to perform evaluation on a diverse set of graphs~\cite{beamer2015locality}.
We use a mix of real-world and synthetic graphs in our evaluation and primarily focus on social network topologies. 

The real-world graphs we use come from a variety of sources. 
We examine two real-world social network graphs: Pokec and LiveJournal which represent the links between users in their online communities. 
We also consider the Hollywood graph which considers professional relationships.
It links together actors that have performed together.
These are all representative of a scale free or social network topology as they all have a low effective diameter and power-law degree distribution.
In contrast, we also use three road network graphs in our evaluations. 
These are mesh topologies with low average degree and high diameter. 

In addition to these real-world graphs, we fill out our set of graphs with four synthetic graphs.
Like in the Graph500 benchmark, we generate several kron graphs of varying size using the \kron generator~\cite{murphy2010graph500}.
These graphs model a social network and follow a power-law degree distribution. 


\begin{table*}[h]
\centering
\begin{tabular}{lllll}
\toprule
\textbf{Name} & \textbf{Description} & \textbf{Vertices} & \textbf{Edges} & \textbf{Degree} \\ \midrule
Kron18 & \kron generated~\cite{leskovec2005realistic,leskovec2010kronecker} & 262,144 & 4,194,304 & 16 \\
Kron19 & \kron generated~\cite{leskovec2005realistic,leskovec2010kronecker} & 524,288 & 8,388,608 & 16 \\
Kron20 & \kron generated~\cite{leskovec2005realistic,leskovec2010kronecker} & 1,048,576 & 16,777,216 & 16 \\
Kron22 & \kron generated~\cite{leskovec2005realistic,leskovec2010kronecker} & 4,194,304 & 67,108,864 & 16 \\
Pokec & social network~\cite{snapnets} & 1,632,803 & 30,622,564 & 18.8 \\
LiveJournal & social network~\cite{mislove2007measurement,davis2011university} & 4,847,571 & 85,702,474 & 17.6 \\
Hollywood & movie collaborations~\cite{boldi2011layered,boldi2004webgraph,davis2011university} & 1,139,905 & 112,751,422 & 98.9\\
RoadCA & CA road network~\cite{davis2011university} & 1,971,281 & 5,533,214 & 2.8\\
RoadCentral & Central road network~\cite{davis2011university} & 14,081,816 & 33,866,826 & 2.4\\
RoadUSA & USA road network~\cite{road-graph} & 23,947,347 & 57,708,624 & 2.4\\
\bottomrule
\end{tabular}
\caption{List of graphs used in this thesis and their properties.All of the graphs come from real-world data except
the three \kron graphs. Throughout our evaluation, we list the subsets of these graphs that are being evaluated.}
\label{tab:graphprop}
\end{table*}

\section{Algorithms used in this Thesis}\label{thesis:background:algorithms}
%\todo{explain bfs, sssp (delta + bellmanford), pr, cc, bc, etc. in detail}
In this thesis we explore a variety of graph processing algorithms. 
We select them based on their popularity in graph-processing evaluations~\cite{beamer2016thesis} and for the different behaviors that they exhibit.
The algorithms we examine can be classified as traversal-centric or compute-centric algorithms. 
Traversal-centric or frontier-based algorithms start from a given source vertex and perform computation on vertices by traversing outwards from the source vertex.
Compute-centric algorithms operate on the entire graph in parallel and tend to iteratively apply updates until the algorithm converges.
Of the algorithms studied in this thesis BFS, SSSP, and BC are traversal-centric algorithms.
PageRank and CC are compute centric.

\paragraph{Breadth First Search (BFS)}\mbox{}\\
BFS is a building block of many graph algorithms. 
It is not an algorithm but really a graph traversal order. 
BFS visits every vertex at a given depth of the graph before moving on to the next depth level.
There has been considerable work done to accelerate BFS and increase the computational work through algorithmic and data structure modification~\cite{agarwal2010scalable,beamer-bfs-direction,bulucc2011parallel,hong2011efficient,yoo2005scalable}
We turn it into an algorithm by discovering and tracking the parent vertex ID of each vertex reachable from a given source vertex.

\paragraph{Single Source Shortest Path (SSSP)}\mbox{}\\
SSSP is an algorithm that builds off of BFS to compute the distances of the shortest paths from a given source vertex to every other reachable vertex in the graph.
This is usually performed on a weighted graph, so the weights of edges are used in calculating the distance of a path.
We only consider graphs with non-negative edge weights in this work.
We examine two different SSSP algorithms, frontier-based Bellman-Ford and Delta-stepping.

Frontier-based Bellman-Ford trades off repeated accesses to edges for increased parallelism. 
It uses relaxation where approximations of the distances to each vertex are replaced by shorter distances until the algorithm converges on the correct solution.
Unlike Dijkstra's, the classical SSSP algorithm, there is no notion of priority in Bellman-Ford and all edges active in the frontier are relaxed in every iteration.

The delta-stepping algorithm~\cite{meyer2003delta} increases parallelism by using a notion of relaxed priority. 
Delta-stepping coarsely sorts the vertices of the graph by distance into buckets of width $\Delta$.
This allows for all vertices in a bucket to be processed in parallel. 
Like Bellman-Ford, this does result in some vertices being processed multiple times, but the frequency with which this occurs can be reduced by reducing the value of $\Delta$.
If $\Delta=1$, the algorithm effectively becomes Dijkstra's, and if $\Delta=\inf$, the algorithm behaves like Bellman-Ford. 

\paragraph{PageRank (PR)}\mbox{}\\
PR calculates the importance or "popularity" of each vertex in a graph and was originally developed to sort web search results~\cite{page1999pagerank}.
It calculates the popularity of a vertex $v$ by considering both the number of vertices that point to $v$ and the importance of those vertices that point to $v$.
PR iteratively updates the PageRank score for each vertex in the graph until the scores converge within some specified tolerance.
It is a topology-driven algorithm where all the edges are traversed in each iteration.
PR also exhibits massive parallelism in each round.
There has been considerable work on optimizing PageRank and finding ways to improve the convergence rate~\cite{low2010graphlab,shun2013ligra,kohlschutter2006efficient,berkhin2005survey}

\paragraph{Connected Components (CC)}\mbox{}\\
The Connected Components algorithm labels all of the components in a graph. 
A connected component is a subgraph in which all vertices are connected to each other.
If an edge exists between two vertices, they are connected. 
In a directed graph, connections between vertices can be asymmetric.
This means that components of a directed graph can be strongly or weakly connected. 
In this thesis, we only consider undirected graphs and do not need to consider asymmetry of connections.
The CC algorithm labels vertices so that all vertices in the same component have the same label.
Like PR, CC is also topology-driven and traverses every edge in each iteration.

\paragraph{Betweenness Centrality (BC)}\mbox{}\\
BC is another algorithm that attempts to measure the importance of the vertices in a graph. 
It calculates a score for each vertex that measures the fraction of shortest paths that pass through the vertex.
This can be computationally expensive as the algorithm needs to compute the shortest paths for all pairs of vertices in the graph.
This is often done by computing the All Pairs Shortest Path algorithm which executes SSSP for every vertex in the graph as a source vertex. 
Calculating all of the shortest paths can be both compute and memory intensive. 
The Brandes algorithm reduces the memory requirements by compacting the critical information from a SSSP execution into a single variable~\cite{brandes2001faster}.
We also compute BC on an unweighted graph in this work, which allows us to use BFS traversals to compute the shortest paths. 
